{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\anaconda\\lib\\site-packages (from sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-5.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss-cpu\n",
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99be47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81476247",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_excel(\"./data.xlsx\")  # data.xlsx\n",
    "fcst  = pd.read_excel(\"./Forcast.xlsx\")  # Forcast.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9089f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 678025 entries, 0 to 678024\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Site              678021 non-null  object \n",
      " 1   Date              678019 non-null  object \n",
      " 2   Invoice Number    678013 non-null  object \n",
      " 3   Customer Code     678002 non-null  object \n",
      " 4   Name              677996 non-null  object \n",
      " 5   Item Code         677993 non-null  object \n",
      " 6   Item Description  677983 non-null  object \n",
      " 7   Quantity          677916 non-null  float64\n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 41.4+ MB\n"
     ]
    }
   ],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65e6fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 72 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   BRAND                24 non-null     object \n",
      " 1   ACTIVE               24 non-null     object \n",
      " 2   Product              24 non-null     object \n",
      " 3   Product description  24 non-null     object \n",
      " 4   PROD                 24 non-null     object \n",
      " 5   TYP                  24 non-null     object \n",
      " 6   STATUS               24 non-null     object \n",
      " 7   ON HAND INITIAL      0 non-null      float64\n",
      " 8   PO INITIAL           0 non-null      float64\n",
      " 9   OCT                  18 non-null     float64\n",
      " 10  NOV                  18 non-null     float64\n",
      " 11  DEC                  24 non-null     int64  \n",
      " 12  JAN                  24 non-null     int64  \n",
      " 13  FEB                  23 non-null     float64\n",
      " 14  MAR                  23 non-null     float64\n",
      " 15  APR                  24 non-null     int64  \n",
      " 16  MAY                  23 non-null     float64\n",
      " 17  JUN                  24 non-null     int64  \n",
      " 18  JUL                  20 non-null     float64\n",
      " 19  AUG                  20 non-null     float64\n",
      " 20  SEPT                 21 non-null     float64\n",
      " 21  OCT.1                16 non-null     float64\n",
      " 22  NOV.1                16 non-null     float64\n",
      " 23  DEC.1                16 non-null     float64\n",
      " 24  JAN.1                22 non-null     float64\n",
      " 25  FEB.1                22 non-null     float64\n",
      " 26  MAR.1                22 non-null     float64\n",
      " 27  APR.1                22 non-null     float64\n",
      " 28  MAY.1                22 non-null     float64\n",
      " 29  JUN.1                22 non-null     float64\n",
      " 30  JULY                 16 non-null     float64\n",
      " 31  AUG.1                23 non-null     float64\n",
      " 32  SEPT.1               22 non-null     float64\n",
      " 33  OCT.2                22 non-null     float64\n",
      " 34  NOV.2                21 non-null     float64\n",
      " 35  DEC.2                22 non-null     float64\n",
      " 36  JAN.2                22 non-null     float64\n",
      " 37  FEB.2                22 non-null     float64\n",
      " 38  MAR.2                22 non-null     float64\n",
      " 39  APR.2                22 non-null     float64\n",
      " 40  MAY.2                22 non-null     float64\n",
      " 41  JUN.2                22 non-null     float64\n",
      " 42  JUL.1                23 non-null     float64\n",
      " 43  AUG.2                23 non-null     float64\n",
      " 44  SEPT.2               22 non-null     float64\n",
      " 45  OCT.3                21 non-null     float64\n",
      " 46  NOV.3                22 non-null     float64\n",
      " 47  DEC.3                22 non-null     float64\n",
      " 48  JAN.3                22 non-null     float64\n",
      " 49  FEB.3                22 non-null     float64\n",
      " 50  MAR.3                23 non-null     float64\n",
      " 51  APR.3                22 non-null     float64\n",
      " 52  MAY.3                22 non-null     float64\n",
      " 53  JUN.3                22 non-null     float64\n",
      " 54  JUL.2                23 non-null     float64\n",
      " 55  AUG.3                22 non-null     float64\n",
      " 56  SEPT.3               22 non-null     float64\n",
      " 57  OCT.4                21 non-null     float64\n",
      " 58  NOV.4                21 non-null     float64\n",
      " 59  DEC.4                23 non-null     float64\n",
      " 60  JAN.4                22 non-null     float64\n",
      " 61  FEB.4                22 non-null     float64\n",
      " 62  MAR.4                22 non-null     float64\n",
      " 63  APR.4                16 non-null     float64\n",
      " 64  MAY.4                16 non-null     float64\n",
      " 65  JUN.4                16 non-null     float64\n",
      " 66  JUL.3                16 non-null     float64\n",
      " 67  AUG.4                16 non-null     float64\n",
      " 68  SEP                  16 non-null     float64\n",
      " 69  OCT.5                16 non-null     float64\n",
      " 70  NOV.5                16 non-null     float64\n",
      " 71  DEC.5                16 non-null     float64\n",
      "dtypes: float64(61), int64(4), object(7)\n",
      "memory usage: 13.6+ KB\n"
     ]
    }
   ],
   "source": [
    "fcst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c7abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only needed columns (robust to case/whitespace)\n",
    "def pick(df, names):\n",
    "    cols = {c.strip().lower(): c for c in df.columns}\n",
    "    out = {}\n",
    "    for n in names:\n",
    "        key = n.strip().lower()\n",
    "        if key in cols: out[n] = cols[key]\n",
    "    return df[[out[n] for n in out]]\n",
    "\n",
    "sales = pick(sales, [\"Site\",\"Date\",\"Invoice Number\",\"Customer Code\",\"Name\",\"Item Code\",\"Item Description\",\"Quantity\"])\n",
    "fcst  = pick(fcst , [\"BRAND\",\"ACTIVE\",\"Product\",\"Product description\",\"PROD\",\"TYP\",\"STATUS\"]) \\\n",
    "        .join(pd.read_excel(\"./Forcast.xlsx\").drop(columns=[\"BRAND\",\"ACTIVE\",\"Product\",\"Product description\",\"PROD\",\"TYP\",\"STATUS\"], errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6de1e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BRAND', 'ACTIVE', 'Product', 'Product description', 'PROD', 'TYP',\n",
       "       'STATUS', 'ON HAND INITIAL', 'PO INITIAL', 'OCT', 'NOV', 'DEC', 'JAN',\n",
       "       'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEPT', 'OCT.1',\n",
       "       'NOV.1', 'DEC.1', 'JAN.1', 'FEB.1', 'MAR.1', 'APR.1', 'MAY.1', 'JUN.1',\n",
       "       'JULY', 'AUG.1', 'SEPT.1', 'OCT.2', 'NOV.2', 'DEC.2', 'JAN.2', 'FEB.2',\n",
       "       'MAR.2', 'APR.2', 'MAY.2', 'JUN.2', 'JUL.1', 'AUG.2', 'SEPT.2', 'OCT.3',\n",
       "       'NOV.3', 'DEC.3', 'JAN.3', 'FEB.3', 'MAR.3', 'APR.3', 'MAY.3', 'JUN.3',\n",
       "       'JUL.2', 'AUG.3', 'SEPT.3', 'OCT.4', 'NOV.4', 'DEC.4', 'JAN.4', 'FEB.4',\n",
       "       'MAR.4', 'APR.4', 'MAY.4', 'JUN.4', 'JUL.3', 'AUG.4', 'SEP', 'OCT.5',\n",
       "       'NOV.5', 'DEC.5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c5808ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct matches: 8\n",
      "678025\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# EDA + Data Sanity + Linkage - \n",
    "# Sets of IDs to compare (case-insensitive)\n",
    "sales[\"item_code_std\"] = sales[\"Item Code\"].astype(str).str.upper().str.strip()\n",
    "fcst[\"product_std\"]    = fcst[\"Product\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# --- Find exact matches ---\n",
    "direct_codes = sorted(set(sales[\"item_code_std\"]) & set(fcst[\"product_std\"]))\n",
    "print(\"Direct matches:\", len(direct_codes))\n",
    "print(len(sales[\"item_code_std\"]))\n",
    "print(len(fcst[\"product_std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08318edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All forcast products - 8 are present in sales, but not all, hence forcast for missing products in Sales will \n",
    "# be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad075c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678025 12611\n"
     ]
    }
   ],
   "source": [
    "neg_rows = sales[sales[\"Quantity\"]<0]\n",
    "print(len(sales), len(neg_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "162a4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building linkage table - \n",
    "link = pd.DataFrame({\"item_code_std\": list(direct_codes)})\n",
    "link = link.merge(fcst[[\"product_std\"]].drop_duplicates(),\n",
    "                  left_on=\"item_code_std\", right_on=\"product_std\", how=\"left\")\n",
    "link = link.rename(columns={\"product_std\":\"product_link\"})\n",
    "assert not link[\"item_code_std\"].duplicated().any(), \"Duplicate key in linkage table!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19d07d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   item_code_std  8 non-null      object\n",
      " 1   product_link   8 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 260.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "link.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e9fb18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check primary key uniqueness ---\n",
    "assert not link[\"item_code_std\"].duplicated().any(), \"Duplicate key in linkage table!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03e58654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows preserved: 678025\n",
      "              Item Code product_link forecast_available  Quantity Order Type\n",
      "0     NBOMOATS16X350GPB          NaN                 No       2.0   Accepted\n",
      "1            HISTV43A4K          NaN                 No      -1.0  Cancelled\n",
      "2  NBOMMULTIGR16X350GPB          NaN                 No      15.0   Accepted\n"
     ]
    }
   ],
   "source": [
    "# Apply linkage -\n",
    "before = len(sales)\n",
    "sales_linked = sales.merge(link[[\"item_code_std\",\"product_link\"]], on=\"item_code_std\", how=\"left\")\n",
    "after  = len(sales_linked)\n",
    "assert before == after, \"Row count changed after merge!\"\n",
    "\n",
    "sales_linked[\"forecast_available\"] = np.where(sales_linked[\"product_link\"].notna(), \"Yes\", \"No\")\n",
    "sales_linked[\"Order Type\"] = np.where(sales_linked[\"Quantity\"] < 0, \"Cancelled\", \"Accepted\")\n",
    "print(\"Rows preserved:\", after)\n",
    "print(sales_linked.head(3)[[\"Item Code\",\"product_link\",\"forecast_available\",\"Quantity\",\"Order Type\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb14cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 678025 entries, 0 to 678024\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Site                678021 non-null  object \n",
      " 1   Date                678019 non-null  object \n",
      " 2   Invoice Number      678013 non-null  object \n",
      " 3   Customer Code       678002 non-null  object \n",
      " 4   Name                677996 non-null  object \n",
      " 5   Item Code           677993 non-null  object \n",
      " 6   Item Description    677983 non-null  object \n",
      " 7   Quantity            677916 non-null  float64\n",
      " 8   item_code_std       678025 non-null  object \n",
      " 9   product_link        42867 non-null   object \n",
      " 10  forecast_available  678025 non-null  object \n",
      " 11  Order Type          678025 non-null  object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 62.1+ MB\n"
     ]
    }
   ],
   "source": [
    "sales_linked.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6accaec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_linked['forecast_available'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d37e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OCT': 'OCT_2025', 'NOV': 'NOV_2025', 'DEC': 'DEC_2025', 'JAN': 'JAN_2026', 'FEB': 'FEB_2026', 'MAR': 'MAR_2026', 'APR': 'APR_2026', 'MAY': 'MAY_2026', 'JUN': 'JUN_2026', 'JUL': 'JUL_2026', 'AUG': 'AUG_2026', 'SEPT': 'SEP_2026'}\n",
      "Index(['BRAND', 'ACTIVE', 'Product', 'Product description', 'PROD', 'TYP',\n",
      "       'STATUS', 'ON HAND INITIAL', 'PO INITIAL', 'OCT_2025', 'NOV_2025',\n",
      "       'DEC_2025', 'JAN_2026', 'FEB_2026', 'MAR_2026', 'APR_2026', 'MAY_2026',\n",
      "       'JUN_2026', 'JUL_2026', 'AUG_2026', 'SEP_2026', 'OCT_2026', 'NOV_2026',\n",
      "       'DEC_2026', 'JAN_2027', 'FEB_2027', 'MAR_2027', 'APR_2027', 'MAY_2027',\n",
      "       'JUN_2027', 'JUL_2027', 'AUG_2027', 'SEP_2027', 'OCT_2027', 'NOV_2027',\n",
      "       'DEC_2027', 'JAN_2028', 'FEB_2028', 'MAR_2028', 'APR_2028', 'MAY_2028',\n",
      "       'JUN_2028', 'JUL_2028', 'AUG_2028', 'SEP_2028', 'OCT_2028', 'NOV_2028',\n",
      "       'DEC_2028', 'JAN_2029', 'FEB_2029', 'MAR_2029', 'APR_2029', 'MAY_2029',\n",
      "       'JUN_2029', 'JUL_2029', 'AUG_2029', 'SEP_2029', 'OCT_2029', 'NOV_2029',\n",
      "       'DEC_2029', 'JAN_2030', 'FEB_2030', 'MAR_2030', 'APR_2030', 'MAY_2030',\n",
      "       'JUN_2030', 'JUL_2030', 'AUG_2030', 'SEP_2030', 'OCT_2030', 'NOV_2030',\n",
      "       'DEC_2030', 'product_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# I have assumed rightmost months as 2025 months and 2024, 2023 and so on as we traverse data leftwards - \n",
    "\n",
    "# Rename forecast month columns per your rule:\n",
    "# - From the LEFTMOST columns up to DEC: treat as 2025 months.\n",
    "# - First JAN from the LEFT = 2026 JAN (and onward months = 2026)\n",
    "# - Second JAN from the LEFT = 2027 JAN (and onward months = 2027)\n",
    "# - And so on.\n",
    "\n",
    "# Assumes `fcst` is already a pandas DataFrame loaded from Forcast.xlsx\n",
    "\n",
    "month_abbr = [\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\",\"JUL\",\"AUG\",\"SEP\",\"OCT\",\"NOV\",\"DEC\"]\n",
    "def is_month_col(c):\n",
    "    return isinstance(c, str) and c.strip().upper()[:3] in month_abbr\n",
    "\n",
    "month_cols = [c for c in fcst.columns if is_month_col(c)]\n",
    "\n",
    "jan_count = 0\n",
    "rename_map = {}\n",
    "for col in month_cols:  # left-to-right\n",
    "    m3 = col.strip().upper()[:3]\n",
    "    if m3 == \"JAN\":\n",
    "        jan_count += 1           # 1st JAN => 2026, 2nd JAN => 2027, etc.\n",
    "    year = 2025 + jan_count      # pre-1st JAN (jan_count=0) => 2025\n",
    "    rename_map[col] = f\"{m3}_{year}\"\n",
    "\n",
    "fcst_renamed = fcst.rename(columns=rename_map)\n",
    "\n",
    "# (optional) quick check\n",
    "print({k: rename_map[k] for k in month_cols[:12]})\n",
    "print(fcst_renamed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8cb83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Keep only forecast columns we want (product key, description, month-year)\n",
    "import re\n",
    "month_col = re.compile(r\"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)_\\d{4}$\", re.I)\n",
    "keep_cols = [\"product_std\", \"Product description\"] + [c for c in fcst_renamed.columns if month_col.match(str(c))]\n",
    "fcst_slim = fcst_renamed[keep_cols].drop_duplicates(subset=[\"product_std\"])\n",
    "\n",
    "# 3) Merge (left) — preserves sales row count; no duplication\n",
    "before = len(sales_linked)\n",
    "enriched = sales_linked.merge(fcst_slim, left_on=\"product_link\", right_on=\"product_std\", how=\"left\")\n",
    "assert len(enriched) == before, \"Row count changed — check keys!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b546004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678025, 77)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98e84690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 678025 entries, 0 to 678024\n",
      "Data columns (total 77 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   Site                 678021 non-null  object \n",
      " 1   Date                 678019 non-null  object \n",
      " 2   Invoice Number       678013 non-null  object \n",
      " 3   Customer Code        678002 non-null  object \n",
      " 4   Name                 677996 non-null  object \n",
      " 5   Item Code            677993 non-null  object \n",
      " 6   Item Description     677983 non-null  object \n",
      " 7   Quantity             677916 non-null  float64\n",
      " 8   item_code_std        678025 non-null  object \n",
      " 9   product_link         42867 non-null   object \n",
      " 10  forecast_available   678025 non-null  object \n",
      " 11  Order Type           678025 non-null  object \n",
      " 12  product_std          42867 non-null   object \n",
      " 13  Product description  42867 non-null   object \n",
      " 14  OCT_2025             42095 non-null   float64\n",
      " 15  NOV_2025             42095 non-null   float64\n",
      " 16  DEC_2025             42867 non-null   float64\n",
      " 17  JAN_2026             42867 non-null   float64\n",
      " 18  FEB_2026             42867 non-null   float64\n",
      " 19  MAR_2026             42867 non-null   float64\n",
      " 20  APR_2026             42867 non-null   float64\n",
      " 21  MAY_2026             42867 non-null   float64\n",
      " 22  JUN_2026             42867 non-null   float64\n",
      " 23  JUL_2026             42867 non-null   float64\n",
      " 24  AUG_2026             42867 non-null   float64\n",
      " 25  SEP_2026             42867 non-null   float64\n",
      " 26  OCT_2026             42867 non-null   float64\n",
      " 27  NOV_2026             42867 non-null   float64\n",
      " 28  DEC_2026             42867 non-null   float64\n",
      " 29  JAN_2027             42867 non-null   float64\n",
      " 30  FEB_2027             42867 non-null   float64\n",
      " 31  MAR_2027             42867 non-null   float64\n",
      " 32  APR_2027             42867 non-null   float64\n",
      " 33  MAY_2027             42867 non-null   float64\n",
      " 34  JUN_2027             42867 non-null   float64\n",
      " 35  JUL_2027             42867 non-null   float64\n",
      " 36  AUG_2027             42867 non-null   float64\n",
      " 37  SEP_2027             42867 non-null   float64\n",
      " 38  OCT_2027             42867 non-null   float64\n",
      " 39  NOV_2027             42867 non-null   float64\n",
      " 40  DEC_2027             42867 non-null   float64\n",
      " 41  JAN_2028             42867 non-null   float64\n",
      " 42  FEB_2028             42867 non-null   float64\n",
      " 43  MAR_2028             42867 non-null   float64\n",
      " 44  APR_2028             42867 non-null   float64\n",
      " 45  MAY_2028             42867 non-null   float64\n",
      " 46  JUN_2028             42867 non-null   float64\n",
      " 47  JUL_2028             42867 non-null   float64\n",
      " 48  AUG_2028             42867 non-null   float64\n",
      " 49  SEP_2028             42867 non-null   float64\n",
      " 50  OCT_2028             42867 non-null   float64\n",
      " 51  NOV_2028             42867 non-null   float64\n",
      " 52  DEC_2028             42867 non-null   float64\n",
      " 53  JAN_2029             42867 non-null   float64\n",
      " 54  FEB_2029             42867 non-null   float64\n",
      " 55  MAR_2029             42867 non-null   float64\n",
      " 56  APR_2029             42867 non-null   float64\n",
      " 57  MAY_2029             42867 non-null   float64\n",
      " 58  JUN_2029             42867 non-null   float64\n",
      " 59  JUL_2029             42867 non-null   float64\n",
      " 60  AUG_2029             42867 non-null   float64\n",
      " 61  SEP_2029             42867 non-null   float64\n",
      " 62  OCT_2029             42867 non-null   float64\n",
      " 63  NOV_2029             42867 non-null   float64\n",
      " 64  DEC_2029             42867 non-null   float64\n",
      " 65  JAN_2030             42867 non-null   float64\n",
      " 66  FEB_2030             42867 non-null   float64\n",
      " 67  MAR_2030             42867 non-null   float64\n",
      " 68  APR_2030             42867 non-null   float64\n",
      " 69  MAY_2030             42867 non-null   float64\n",
      " 70  JUN_2030             42867 non-null   float64\n",
      " 71  JUL_2030             42867 non-null   float64\n",
      " 72  AUG_2030             42867 non-null   float64\n",
      " 73  SEP_2030             42867 non-null   float64\n",
      " 74  OCT_2030             42867 non-null   float64\n",
      " 75  NOV_2030             42867 non-null   float64\n",
      " 76  DEC_2030             42867 non-null   float64\n",
      "dtypes: float64(64), object(13)\n",
      "memory usage: 398.3+ MB\n"
     ]
    }
   ],
   "source": [
    "enriched.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "707cf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping parquet file for simpler chunking - \n",
    "# Convert all object columns to string to avoid ArrowTypeError\n",
    "for col in enriched.select_dtypes(include=\"object\").columns:\n",
    "    enriched[col] = enriched[col].astype(str)\n",
    "\n",
    "enriched.to_parquet(\"enriched.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2f0cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(678025, 78)\n",
      "        Date             Item Code  Quantity month_key\n",
      "0 2024-01-02     NBOMOATS16X350GPB       2.0  JAN_2024\n",
      "1 2024-01-02            HISTV43A4K      -1.0  JAN_2024\n",
      "2 2024-01-02  NBOMMULTIGR16X350GPB      15.0  JAN_2024\n",
      "3 2024-01-02  LILYULTRASOFTNR10X12       1.0  JAN_2024\n",
      "4 2024-01-02  NBOMMBANAPL16X350GPB      10.0  JAN_2024\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for chunking - \n",
    "\n",
    "merged = pd.read_parquet(\"enriched.parquet\")\n",
    "merged[\"Date\"] = pd.to_datetime(merged[\"Date\"], errors=\"coerce\")\n",
    "merged[\"month_key\"] = merged[\"Date\"].dt.strftime(\"%b_%Y\").str.upper()\n",
    "print(merged.shape)\n",
    "print(merged[[\"Date\",\"Item Code\",\"Quantity\",\"month_key\"]].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2747689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks rebuilt: (678025, 6)\n"
     ]
    }
   ],
   "source": [
    "# Building text chunks + metadata =>\n",
    "\n",
    "# Keep only rows where forecast product exists (8 SKUs)\n",
    "subset = merged[merged[\"product_std\"].notna()].copy()\n",
    "print(\"Subset shape:\", subset.shape)\n",
    "\n",
    "# find forecast month columns like JAN_2025, FEB_2026, ...\n",
    "month_re = re.compile(r\"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)_(\\d{4})$\", re.I)\n",
    "month_cols = [c for c in subset.columns if month_re.match(str(c))]\n",
    "\n",
    "# group by month abbrev (choose the earliest year, e.g., 2025)\n",
    "from collections import defaultdict\n",
    "month_groups = defaultdict(list)\n",
    "for c in month_cols:\n",
    "    m, y = month_re.match(c).groups()\n",
    "    month_groups[m.upper()].append((int(y), c))\n",
    "for m in month_groups:\n",
    "    month_groups[m].sort()  # ascending by year\n",
    "\n",
    "def pick_forecast_any_year(row):\n",
    "    # use month name only (ignore sales year)\n",
    "    mk = str(row.get(\"month_key\",\"\"))\n",
    "    mon = mk.split(\"_\")[0] if \"_\" in mk else None\n",
    "    if mon and mon.upper() in month_groups:\n",
    "        # pick earliest year for that month (e.g., JAN_2025)\n",
    "        return row.get(month_groups[mon.upper()][0][1])\n",
    "    return np.nan\n",
    "\n",
    "def row_to_chunk(row):\n",
    "    date_str = pd.to_datetime(row[\"Date\"], errors=\"coerce\")\n",
    "    date_str = date_str.date().isoformat() if pd.notna(date_str) else \"NA\"\n",
    "    fcst_val = pick_forecast_any_year(row)\n",
    "    fcst_txt = \"N/A\" if pd.isna(fcst_val) else str(fcst_val)\n",
    "    return {\n",
    "        \"text\": (\n",
    "            f\"Product: {row.get('Item Code')}; \"\n",
    "            f\"Site: {row.get('Site')}; \"\n",
    "            f\"Date: {date_str}; \"\n",
    "            f\"Month: {row.get('month_key')}; \"\n",
    "            f\"Sales Qty: {row.get('Quantity')}; \"\n",
    "            f\"Forecast: {fcst_txt}; \"\n",
    "            f\"Order Type: {row.get('Order Type','Accepted')}\"\n",
    "        ),\n",
    "        \"item_code\": row.get(\"Item Code\"),\n",
    "        \"site\": row.get(\"Site\"),\n",
    "        \"date\": date_str,\n",
    "        \"month_key\": row.get(\"month_key\"),\n",
    "        \"source\": \"merged.parquet\"\n",
    "    }\n",
    "\n",
    "chunks = pd.DataFrame([row_to_chunk(r) for _, r in merged.iterrows()])\n",
    "chunks.to_parquet(\"chunks.parquet\", index=False)\n",
    "print(\"chunks rebuilt:\", chunks.shape)\n",
    "\n",
    "# save for embedding step - \n",
    "chunks.to_parquet(\"chunks.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9c383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b4db91f2114de79d1ac01b6fdbe5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed: 678025 vectors; dim: 384\n"
     ]
    }
   ],
   "source": [
    "# # Embeddings (MiniLM, CPU) → FAISS index + metadata\n",
    "\n",
    "# import faiss\n",
    "# from sentence_transformers import SentenceTransformer # type: ignore\n",
    "\n",
    "# # 1) Load chunks\n",
    "# chunks = pd.read_parquet(\"chunks.parquet\")\n",
    "\n",
    "# # 2) Encode text → vectors (384-dim, CPU)\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # small & fast on CPU\n",
    "# emb = model.encode(chunks[\"text\"].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True)\n",
    "# emb = np.asarray(emb, dtype=\"float32\")\n",
    "\n",
    "# # 3) FAISS index (cosine via inner product on normalized vectors)\n",
    "# index = faiss.IndexFlatIP(emb.shape[1])\n",
    "# index.add(emb)\n",
    "\n",
    "# # 4) Save index + metadata\n",
    "# faiss.write_index(index, \"faiss.index\")\n",
    "# chunks.reset_index(drop=True).to_parquet(\"meta.parquet\", index=False)\n",
    "\n",
    "# print(\"Indexed:\", index.ntotal, \"vectors; dim:\", emb.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "004a55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chunks: 678025\n",
      "num_chunks: 473670\n"
     ]
    }
   ],
   "source": [
    "print(\"num_chunks:\", len(chunks))\n",
    "chunks1 = chunks.drop_duplicates(\"text\")\n",
    "print(\"num_chunks:\", len(chunks1))\n",
    "chunks[\"text\"] = chunks[\"text\"].str.slice(0, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98793ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45026509ac504b2585179c4946e158b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index ready for forecast subset: 473670\n"
     ]
    }
   ],
   "source": [
    "# 4) Embeddings - \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb = model.encode(chunks1[\"text\"].tolist(), batch_size=512,\n",
    "                   show_progress_bar=True, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "# 5) FAISS index\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "faiss.write_index(index, \"faiss_forecast.index\")\n",
    "chunks1.to_parquet(\"meta_forecast.parquet\", index=False)\n",
    "\n",
    "print(\"FAISS index ready for forecast subset:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retriever: embed query → search FAISS → return top-k chunks + metadata (CPU-only)\n",
    "\n",
    "# import faiss, numpy as np, pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # 1) Load FAISS + metadata (from 2.3)\n",
    "# index = faiss.read_index(\"faiss.index\")\n",
    "# meta  = pd.read_parquet(\"meta.parquet\")  # contains columns: text, item_code, site, month_key, source, ...\n",
    "\n",
    "# # 2) Load encoder (same as used for indexing)\n",
    "# enc = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# def retrieve(query: str, k: int = 5):\n",
    "#     # 3) Encode query (normalize for cosine/IP)\n",
    "#     q = enc.encode([query], normalize_embeddings=True)\n",
    "#     q = np.asarray(q, dtype=\"float32\")\n",
    "#     # 4) Search\n",
    "#     scores, idx = index.search(q, k)         # scores shape: (1,k), idx shape: (1,k)\n",
    "#     idx = idx[0].tolist(); scores = scores[0].tolist()\n",
    "#     # 5) Pack results with metadata\n",
    "#     out = meta.iloc[idx].copy()\n",
    "#     out[\"score\"] = scores\n",
    "#     return out.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a52bac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast-only retriever: uses faiss_forecast.index + meta_forecast.parquet\n",
    "\n",
    "import faiss, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1) Load index + metadata (built for the 8 forecast SKUs)\n",
    "index = faiss.read_index(\"faiss_forecast.index\")\n",
    "meta  = pd.read_parquet(\"meta_forecast.parquet\")  # columns: text, item_code, site, month_key, source\n",
    "\n",
    "# 2) Load the same encoder used during indexing\n",
    "enc = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_forecast(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    # Encode query (normalize for cosine/IP)\n",
    "    q = enc.encode([query], normalize_embeddings=True)\n",
    "    q = np.asarray(q, dtype=\"float32\")\n",
    "    # Search\n",
    "    scores, idx = index.search(q, k)\n",
    "    idx = idx[0].tolist(); scores = scores[0].tolist()\n",
    "    # Pack results\n",
    "    out = meta.iloc[idx].copy()\n",
    "    out[\"score\"] = scores\n",
    "    return out.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e520225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score      item_code  site month_key  \\\n",
      "0  0.608307  HISSPL10HPINV  BBEN  FEB_2024   \n",
      "1  0.605483  HISSPL10HPINV  BAS1  FEB_2024   \n",
      "2  0.604496  HISSPL10HPINV  BAS1  FEB_2024   \n",
      "3  0.603777  HISSPL10HPINV  BAS1  FEB_2024   \n",
      "4  0.603150  HISSPL10HPINV  MIKE  JUN_2024   \n",
      "\n",
      "                                                text  \n",
      "0  Product: HISSPL10HPINV; Site: BBEN; Date: 2024...  \n",
      "1  Product: HISSPL10HPINV; Site: BAS1; Date: 2024...  \n",
      "2  Product: HISSPL10HPINV; Site: BAS1; Date: 2024...  \n",
      "3  Product: HISSPL10HPINV; Site: BAS1; Date: 2024...  \n",
      "4  Product: HISSPL10HPINV; Site: MIKE; Date: 2024...  \n"
     ]
    }
   ],
   "source": [
    "# # Tiny test:\n",
    "q = \"Compare sales vs forecast for HISSPL10HPINV in JAN 2025\"\n",
    "print(retrieve_forecast(q, k=5)[[\"score\",\"item_code\",\"site\",\"month_key\",\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4b31bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt builder -\n",
    "\n",
    "import requests\n",
    "\n",
    "# 1) Build context from retrieved rows (add simple citations)\n",
    "def build_context(rows):\n",
    "    lines = []\n",
    "    for i, r in rows.iterrows():\n",
    "        cite = f\"[CIT{i+1}: {r.get('item_code','NA')} | {r.get('site','NA')} | {r.get('month_key','NA')}]\"\n",
    "        lines.append(f\"{cite} {r['text']}\")\n",
    "    return \"\\n\".join(lines), [f\"CIT{i+1}\" for i in range(len(rows))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aba44929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Ollama generator - Ask Ollama with strict grounding\n",
    "def ask_ollama(question, rows, model=\"llama3.2:latest\", max_tokens=250, temperature=0.2):\n",
    "    context, cites = build_context(rows)\n",
    "    prompt = (\n",
    "        \"SYSTEM: Answer ONLY using the CONTEXT. If info is missing, say 'Not available'. \"\n",
    "        \"Be concise and include citation IDs (e.g., [CIT1]).\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\"\n",
    "    )\n",
    "    r = requests.post(\"http://localhost:11434/api/generate\", json={\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"num_predict\": max_tokens,\n",
    "        \"stream\": False\n",
    "    }, timeout=120)\n",
    "    return r.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c883660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available.\n"
     ]
    }
   ],
   "source": [
    "# --- tiny example flow ---\n",
    "q = \"Compare sales vs forecast for HISSPL10HPINV in JAN_2027.\"\n",
    "topk = retrieve_forecast(q, k=5)\n",
    "print(ask_ollama(q, topk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f8887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59018172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 10:31:21.473 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Anaconda\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-29 10:31:21.475 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7bbbd506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'created_at': '2025-08-29T05:10:54.7626003Z', 'response': '*ping*', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 10194, 128009, 128006, 78191, 128007, 271, 9, 10194, 9], 'total_duration': 6205370600, 'load_duration': 4886905400, 'prompt_eval_count': 26, 'prompt_eval_duration': 1017000000, 'eval_count': 4, 'eval_duration': 281000000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.post(\"http://127.0.0.1:11434/api/generate\",\n",
    "                    json={\"model\":\"llama3.2:latest\",\"prompt\":\"ping\",\"stream\":False}).json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run:   streamlit run app.py\n",
    "# Needs: ollama serve  (and model pulled), faiss_forecast.index, meta_forecast.parquet in working dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
